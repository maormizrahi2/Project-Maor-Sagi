{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23635946-7c99-47bc-b929-a36e44ad7515",
   "metadata": {},
   "source": [
    "# Libraries declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4e095-9285-46ff-a62c-2e7d811a7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import json\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1e4c6-be0c-4935-a88e-9278b53e1a78",
   "metadata": {},
   "source": [
    "***Getting all product categories:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c988945-6bdd-49e2-ad5b-6f799858ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_URL = \"https://www.pricez.co.il/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "page = requests.get(main_URL, headers=headers)\n",
    "print(page.status_code)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "category_URL_arr1 = soup.find_all('div', attrs={'onclick': re.compile('window.location*')})\n",
    "category_URL_arr2 = [link.attrs['onclick'] for link in category_URL_arr1]\n",
    "del category_URL_arr2[-3:-1]  # those 2 categories are not needed (not a category)\n",
    "category_URL_arr3 = {link[link.rindex('/')+1:-1] : main_URL + link[link.index(\"/Category/\"): -1] for link in category_URL_arr2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626e08c-79a6-40f9-ba15-7e5da42e87ea",
   "metadata": {},
   "source": [
    "***Getting all tabs for each categories:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6984784-a0b9-4164-8ba5-dc0c66d4e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Key, URL in category_URL_arr3.items():\n",
    "    page = requests.get(URL , headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    tab_URL_arr_temp1 = soup.find_all('a', attrs={'href': re.compile('#Tab*')})\n",
    "    tab_URL_arr_temp2 = [URL + link.attrs['href'] for link in tab_URL_arr_temp1]\n",
    "    category_URL_arr3[Key] = {tab_URL_arr_temp1[index].text : tab_URL_arr_temp2[index] for index in range(len(tab_URL_arr_temp2))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dac982-d354-453c-9283-5456cb61f821",
   "metadata": {},
   "source": [
    "***Getting all sub tabs under each tab for each categories:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff05a5-2a59-4610-aa9d-941c17227668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Key_Layer1, URL_Layer1 in category_URL_arr3.items():\n",
    "    for Key_Layer2, URL_Layer2 in category_URL_arr3[Key_Layer1].items():\n",
    "        URL = URL_Layer2\n",
    "        page = requests.get(URL, headers=headers)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        sub_tab_URL_arr_temp1 = soup.find_all('div', attrs={'id': re.compile(URL[URL.rindex('#')+1:])})\n",
    "        sub_tab_URL_arr_temp1 = sub_tab_URL_arr_temp1[0].find_all('a', attrs={'href': re.compile('/Category/*')})\n",
    "        category_URL_arr3[Key_Layer1][Key_Layer2] = {link.text: main_URL + link.attrs['href'] for link in sub_tab_URL_arr_temp1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0110916-1a9d-4a72-9fea-5b8ce8429fbc",
   "metadata": {},
   "source": [
    "***Getting all products sub tab for each categories:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57046c75-ab82-4385-bcfa-9d6456d02104",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Key_Layer1, URL_Layer1 in category_URL_arr3.items():\n",
    "    for Key_Layer2, URL_Layer2 in category_URL_arr3[Key_Layer1].items():\n",
    "        for Key_Layer3, URL_Layer3 in category_URL_arr3[Key_Layer1][Key_Layer2].items():\n",
    "            page = requests.get(URL_Layer3, headers=headers)\n",
    "            sleep(randint(2, 5))\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            prod_URL_arr_temp1 = soup.find_all('div', attrs={'class': re.compile('ProductName*')})\n",
    "            prod_URL_arr_temp2 = list()\n",
    "            for temp1 in prod_URL_arr_temp1:\n",
    "                prod_URL_arr_temp2.append(temp1.find_all('a', attrs={'href': re.compile('/Product/*')}))\n",
    "            category_URL_arr3[Key_Layer1][Key_Layer2][Key_Layer3] = {link[0].text: main_URL + link[0].attrs['href'] for link in prod_URL_arr_temp2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213bb35e-39da-4bde-b3ec-316ddcbf2e8e",
   "metadata": {},
   "source": [
    "***Getting all information of all the products:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c39f5-d784-4cf7-942f-6933b9de1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Key_Layer1, URL_Layer1 in category_URL_arr3.items():\n",
    "    for Key_Layer2, URL_Layer2 in category_URL_arr3[Key_Layer1].items():\n",
    "        for Key_Layer3, URL_Layer3 in category_URL_arr3[Key_Layer1][Key_Layer2].items():\n",
    "            sleep(randint(45,60))\n",
    "            for Key_Layer4, URL_Layer4 in category_URL_arr3[Key_Layer1][Key_Layer2][Key_Layer3].items():\n",
    "                page = requests.get(URL_Layer4, headers=headers)\n",
    "                sleep(randint(3, 6))\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                prod_arr1 = soup.find_all('iframe')\n",
    "                prod_arr1 = [attr.attrs['src'] for attr in prod_arr1]\n",
    "                URL = main_URL[:-1] + prod_arr1[0]\n",
    "                page = requests.get(URL)\n",
    "                sleep(randint(2, 4))\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                prod_arr2 = soup.find_all('script', attrs={'type': re.compile(\"text/javascript\")})\n",
    "                prod_arr2 = str(prod_arr2[1])\n",
    "                prod_arr2 = prod_arr2[prod_arr2.index('([[')+1 : prod_arr2.index(']);')-3]+']'\n",
    "                prod_arr2 = literal_eval(prod_arr2)\n",
    "                for i in range(len(prod_arr2)):\n",
    "                    prod_arr2[i][2] = prod_arr2[i][2][prod_arr2[i][2].index('\\n')+2:]\n",
    "                    prod_arr2[i][4] = prod_arr2[i][4][prod_arr2[i][4].index('\\n')+2:]\n",
    "                    prod_arr2[i] = prod_arr2[i][0].split(\"-\") + prod_arr2[i][1:]\n",
    "                category_URL_arr3[Key_Layer1][Key_Layer2][Key_Layer3][Key_Layer4] = prod_arr2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a57a9-265c-4456-9130-7c6d468d4d57",
   "metadata": {},
   "source": [
    "***Put all data to CSV file:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c40a8-36b2-44fa-82aa-b604c52b3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"Category\",     \"Tab\",      \"Sub Tub\",      \"Products\",\n",
    "                \"Year\",         \"Month\",    \"High_Price\",   \"High_Place\", \"Low_Price\", \"Low_Place\"]\n",
    "df = pd.DataFrame(columns=column_names)\n",
    "for Key_Layer1 in category_URL_arr3.keys():\n",
    "    for Key_Layer2 in category_URL_arr3[Key_Layer1].keys():\n",
    "        for Key_Layer3 in category_URL_arr3[Key_Layer1][Key_Layer2].keys():\n",
    "            for Key_Layer4 in category_URL_arr3[Key_Layer1][Key_Layer2][Key_Layer3]:\n",
    "                for data in category_URL_arr3[Key_Layer1][Key_Layer2][Key_Layer3][Key_Layer4]:\n",
    "                    data_dict = {column_names[-1*len(data)+index]: data[index] for index in range(len(data))}\n",
    "                    df = df.append(data_dict, ignore_index=True)\n",
    "                df[column_names[-7]] = df[column_names[-7]].fillna(Key_Layer4)\n",
    "            df[column_names[-8]] = df[column_names[-8]].fillna(Key_Layer3)\n",
    "        df[column_names[-9]] = df[column_names[-9]].fillna(Key_Layer2)\n",
    "    df[column_names[-10]] = df[column_names[-10]].fillna(Key_Layer1)\n",
    "pd.DataFrame(df).to_csv('./product_file.csv', encoding=\"ISO-8859-8\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
